---
title: "R Notebook"
output: html_notebook
---
```{r}
#Load libraries
library(rpart)
library(rpart.plot)
library(randomForest)
library(class)
library(e1071)
library(FNN)
```



```{r}
#read data
originalKepplerData = read.csv("cumulative.csv") #read data
#View(head(originalKepplerData))

factored_keppler_data = originalKepplerData
factored_keppler_data$koi_disposition = factor(originalKepplerData$koi_disposition) #factor character data
factored_keppler_data$koi_pdisposition = factor(originalKepplerData$koi_pdisposition)
head(factored_keppler_data)
remove_KOI_tech_factoered = subset(factored_keppler_data, select = -c(koi_teq_err1)) #remove all-NA columns
remove_KOI_tech_factoered = subset(remove_KOI_tech_factoered, select = -c(koi_teq_err2))

#View(remove_KOI_tech_factoered)
remove_NAs = na.exclude(remove_KOI_tech_factoered) #remove NAs

identifiers_removed = subset(remove_NAs, select = -c(rowid, kepid, kepoi_name, kepler_name, koi_pdisposition, koi_score))
#get rid of unique identifiers, as well as ones that can only be calculated once you know the outcome, like Koi_Pdisposition and koi_score

#Also needs to remove flags, these can only be known after status is confirmed
identifiers_removed = subset(identifiers_removed, select = -c(koi_fpflag_ss, koi_fpflag_ec, koi_fpflag_co, koi_fpflag_nt, koi_tce_plnt_num, koi_tce_delivname))



#separate into labeled and unlabeled data
candidates_final = identifiers_removed[identifiers_removed$koi_disposition ==
                                         "CANDIDATE", ] #separate out just the candidates
labeled_final = identifiers_removed[identifiers_removed$koi_disposition !=
                                      "CANDIDATE", ]
labeled_final = droplevels(labeled_final)
candidates_final = droplevels(candidates_final)
summary(labeled_final)
summary(candidates_final)
```
First, do all models that don't require normalization
```{r}
decTree_Error = rep(0, 5)
RF_error = rep(0, 5)
SVM_error = rep(0, 5)
GLM_error = rep(0, 5)
for (fold in 1:5) {
  set.seed(fold)
  
  num_samples = dim(labeled_final)[1]
  sampling.rate = 0.8
  training = sample(1:num_samples, sampling.rate * num_samples, replace = FALSE)
  trainingSet = subset(labeled_final[training, ])
  testing = setdiff(1:num_samples, training)
  testingSet = subset(labeled_final[testing,])
  
  decTreeModel = rpart(koi_disposition ~ ., data = trainingSet)
  
  
  
  
  #Automatically select the stopping point where cp no longer improves error by 0.05
  errors = decTreeModel$cptable[, 3]
  decTreeChangeError = c(0, 0, 0, 0, 0, 0, 0, 0, 0)
  for (i in 1:8) {
    decTreeChangeError[i] = errors[i + 1] - errors[i]
  }
  decTreeChangeError
  for (i in 1:9) {
    if (abs(decTreeChangeError[i]) < 0.05) {
      stopIndex = i
      break
    }
  }
  
  cps = decTreeModel$cptable[, 1]
  cpStop = cps[stopIndex]
  cpStop
  prunedDecTreeModel = rpart::prune(decTreeModel, cp = cpStop)
  
  decTreePredictions = predict(prunedDecTreeModel, testingSet, type = "class")
  sizeTestSet = dim(testingSet)[1]
  
  decTreeModel_error = sum(decTreePredictions != testingSet$koi_disposition)
  
  misclassificationRateDecTree = decTreeModel_error / sizeTestSet
  #new do pruning, etc.
  
  decTree_Error[fold] = misclassificationRateDecTree
  
  #RandForestModel
  RandForestModel = randomForest(koi_disposition ~ ., data = trainingSet)
  predictedLabels = predict(RandForestModel, testingSet)
  sizeTestSet = dim(testingSet)[1]

  error = sum(predictedLabels != testingSet$koi_disposition)
  misclassification_rateRandForest = error / sizeTestSet
  RF_error[fold] = misclassification_rateRandForest
  
  
  #SVM Model
  svmModel = svm(koi_disposition ~ ., data = trainingSet, kernel = "linear")
  predictedlabelsSVM = predict(svmModel, testingSet)
  errorSVM = sum(predictedlabelsSVM != testingSet$koi_disposition)
  misclassification_rateSVM = errorSVM / sizeTestSet
  SVM_error[fold] = misclassification_rateSVM
  
  
  #Logistic Regression Model
  LogisticReg = glm(koi_disposition ~ .,
                    data = trainingSet,
                    family = binomial(logit))
  predicions = predict(LogisticReg, testingSet, type = "response")
  predictedLabels = rep(0, sizeTestSet)
  predictedLabels = ifelse(predicions > 0.5, 'FALSE POSITIVE', 'CONFIRMED')
  error = sum(predictedLabels != testingSet$koi_disposition)
  misclassificationRateLR = error / sizeTestSet
  GLM_error[fold] = misclassificationRateLR
  

}
```
Calculate Average Errors
```{r}
AvgErrorDT = mean(decTree_Error)
AvgErrorRF = mean(RF_error)
AvgErrorSVM = mean(SVM_error)
AvgErrorGLM = mean(GLM_error)
```


Neural Network:
```{r}
head(labeled_final)
dim(labeled_final)


#Neural network,not doing k-cross fold due to computational limits
set.seed(123)
scaled_data = data.frame(scale(labeled_final[, 2:36]))
scaled_data$koi_disposition = ifelse(labeled_final$koi_disposition == "CONFIRMED", 1,0)
summary(scaled_data)
head(scaled_data)
num_samples = dim(scaled_data)[1]
sampling.rate = 0.8
training = sample(1:num_samples, sampling.rate * num_samples, replace = FALSE)
trainingSet.norm = subset(scaled_data[training, ])
testing = setdiff(1:num_samples, training)
testingSet.norm = subset(scaled_data[testing,])
sizeTestSet = dim(testingSet.norm)[1]
koiDP.name = "koi_disposition"
variable.names = rep(0, numCols)


numCols = dim(testingSet.norm)[2]
variable.names = colnames(testingSet.norm)[1:numCols - 1]

variable.names
for (i in 1:length(variable.names)) {
  error = grepl("_err", variable.names[i], fixed = TRUE)
  if (error) {
    variable.names[i] = NA
  }
  
}

variable.names = na.omit(variable.names)
variable.names = variable.names[1:15]
variable.names

library(formulaic)
nn.form <-
  create.formula(outcome.name = koiDP.name,
                 input.names = variable.names)
nn.form
library(neuralnet)
nnModel2 = neuralnet(
  nn.form,
  data = trainingSet.norm,
  hidden = 2,
  #4,2 does not converge #5,1 does not converge, #6 did not converge, 3,1 logstics did not converge
  #2 logsitic converges!!!!
  linear.output = FALSE,
  act.fct = "logistic"
)

predictedLabels = compute(nnModel2, testingSet.norm[,variable.names])
#predictedLabels$net.result

results = data.frame(actual = testingSet.norm$koi_disposition, prediction = predictedLabels$net.result)
error = sum(results$actual != round(results$prediction))
NeuralNetMisClassRate = error/sizeTestSet
```
KNN:
```{r}
numKs = 5
k_errors = rep(0,numKs)
for (ki in 1:numKs) {
  avgErrors_fold = rep(0,5)
  for (fold in 1:5) {
    set.seed(fold)
    scaled_data = data.frame(scale(labeled_final[, 2:36]))
    scaled_data$koi_disposition = ifelse(labeled_final$koi_disposition == "CONFIRMED", 1, 0)
    summary(scaled_data)
    head(scaled_data)
    num_samples = dim(scaled_data)[1]
    sampling.rate = 0.8
    training = sample(1:num_samples, sampling.rate * num_samples, replace = FALSE)
    trainingSet.norm = subset(scaled_data[training,])
    testing = setdiff(1:num_samples, training)
    testingSet.norm = subset(scaled_data[testing, ])
    sizeTestSet = dim(testingSet.norm)[1]
    
    trainingfeatures = subset(trainingSet.norm, select = c(-koi_disposition))
    traininglabels = trainingSet.norm$koi_disposition
    testingfeatures = subset(testingSet.norm, select = c(-koi_disposition))
    testinglabels = testingSet.norm$koi_disposition
    predictedLabels = knn(trainingfeatures, testingfeatures, traininglabels, k =
                           ki)
 
    error = sum(predictedLabels != testingSet.norm$koi_disposition)
    
    misclassification_rate = error / sizeTestSet
    avgErrors_fold[fold] = misclassification_rate
  }
  
  k_errors[ki] = mean(avgErrors_fold)
}

print(order(k_errors))
#The lowest average error (this run) is from the model with k = 4. 
AvgError_best_knn = k_errors[order(k_errors)[1]]
```
```{r}
library("factoextra")

  scaled_data = data.frame(scale(labeled_final[, 2:36]))
  scaled_data$koi_disposition = ifelse(labeled_final$koi_disposition == "CONFIRMED", 2, 1)
  summary(scaled_data)
  head(scaled_data)
  num_samples = dim(scaled_data)[1]
  features = subset(scaled_data, select = c(-koi_disposition))
  summary(features)
  head(scaled_data)
  kclustering = kmeans(features, centers = 2, nstart = 25)
  
  fviz_cluster(kclustering, data = features)
  head(kclustering$cluster)
  
  error = sum(scaled_data$koi_disposition != kclustering$cluster)
  error
  misclassification_rate = error / dim(scaled_data)[1]
  AvgErrorClustering = misclassification_rate

  



```

```{r}
error_output = data.frame("Model"= c("Decision Tree","GLM","Random Forest","SVM","Neural Net","KNN","Clustering"), "Misclassification Rate" = c(AvgErrorDT,AvgErrorGLM,AvgErrorRF,AvgErrorSVM,NeuralNetMisClassRate,AvgError_best_knn,AvgErrorClustering))
print(error_output)
```

 
 